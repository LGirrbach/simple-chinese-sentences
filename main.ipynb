{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 14460 dialogues\n"
     ]
    }
   ],
   "source": [
    "# Extract dialogues\n",
    "dialogues = []\n",
    "for _, split_data in dataset.items():\n",
    "    split_dialogues = [dialog['dialogue'] for dialog in split_data]\n",
    "    dialogues.extend(split_dialogues)\n",
    "\n",
    "print(f\"Collected {len(dialogues)} dialogues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences: 100%|██████████| 14460/14460 [00:02<00:00, 4969.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 151108 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Extract dialog turns and split sentences\n",
    "sentences = set()\n",
    "for dialogue in tqdm(dialogues, desc=\"Extracting sentences\"):\n",
    "    turns = dialogue.split('\\n')\n",
    "    for turn in turns:\n",
    "        turn = re.sub(r\"#.*?#:\", \"\", turn).strip()\n",
    "        sentences.update(nltk.sent_tokenize(turn))\n",
    "\n",
    "print(f\"Collected {len(sentences)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lgirrbach15/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lgirrbach15/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lgirrbach15/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Filtering sentences: 100%|██████████| 151108/151108 [00:06<00:00, 24265.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 64627 filtered sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# Filter sentences on surface criteria\n",
    "with open(\"resources/english_words.txt\", \"r\") as f:\n",
    "    english_words = set([word.strip() for word in f.read().splitlines()])\n",
    "\n",
    "filtered_sentences = []\n",
    "lowercase_sentences = set()\n",
    "\n",
    "for sentence in tqdm(list(sentences), desc=\"Filtering sentences\"):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    if len(tokens) < 3:\n",
    "        continue\n",
    "    if len(tokens) > 7:\n",
    "        continue\n",
    "\n",
    "    # Filter sentences where at least one word is not in the English word list\n",
    "    if not any(token in english_words for token in tokens):\n",
    "        continue\n",
    "\n",
    "    if sentence.lower() in lowercase_sentences: \n",
    "        continue\n",
    "\n",
    "    filtered_sentences.append(sentence)\n",
    "    lowercase_sentences.add(sentence.lower())\n",
    "\n",
    "print(f\"Collected {len(filtered_sentences)} filtered sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building lemma to sentences mapping: 100%|██████████| 64627/64627 [00:10<00:00, 6226.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 53779 filtered sentences\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build inverse mapping from lemmas to sentences\n",
    "lemma_to_sentences = defaultdict(set)\n",
    "sentences = list(filtered_sentences)\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for sentence in tqdm(sentences, desc=\"Building lemma to sentences mapping\"):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lemmas = set([lemmatizer.lemmatize(token) for token in tokens])\n",
    "    # Filter out stopwords\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in stopwords]\n",
    "\n",
    "    # Get intersection of sentences with the same lemmas\n",
    "    intersection = set.intersection(*[lemma_to_sentences[lemma] for lemma in lemmas])\n",
    "    if len(intersection) > 0:\n",
    "        continue\n",
    "\n",
    "    for lemma in lemmas:\n",
    "        lemma_to_sentences[lemma].add(sentence)\n",
    "\n",
    "filtered_sentences = list(set.union(*lemma_to_sentences.values()))\n",
    "print(f\"Collected {len(filtered_sentences)} filtered sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sentences = list(filtered_sentences)\n",
    "sentence_embeddings = model.encode(sentences, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings for cosine distance\n",
    "import numpy as np\n",
    "sentence_embeddings = sentence_embeddings / np.linalg.norm(sentence_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building clusters:  62%|██████▏   | 33600/53779 [00:10<00:05, 3738.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 33734 clusters\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "processed_sentence_ids = set()\n",
    "batch_size = 100\n",
    "\n",
    "progress_bar = tqdm(total=len(sentences), desc=\"Building clusters\")\n",
    "start_idx = 0\n",
    "\n",
    "while start_idx < len(sentences):\n",
    "    batch_idxs = []\n",
    "    while len(batch_idxs) < batch_size:\n",
    "        if start_idx >= len(sentences):\n",
    "            break\n",
    "        if start_idx in processed_sentence_ids:\n",
    "            start_idx += 1\n",
    "            continue\n",
    "\n",
    "        batch_idxs.append(start_idx)\n",
    "        start_idx += 1\n",
    "\n",
    "    cosine_similarities = np.dot(sentence_embeddings, sentence_embeddings[batch_idxs].T).T\n",
    "    cluster_mask = cosine_similarities > 0.7\n",
    "    for sentence_idx, mask in zip(batch_idxs, cluster_mask):\n",
    "        mask_idxs = np.nonzero(mask)[0].tolist()\n",
    "        mask_idxs = [idx for idx in mask_idxs if idx not in processed_sentence_ids]\n",
    "        if len(mask_idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        clusters.append(mask_idxs)\n",
    "        processed_sentence_ids.update(set(mask_idxs))\n",
    "\n",
    "    progress_bar.update(len(batch_idxs))\n",
    "\n",
    "print(f\"Collected {len(clusters)} clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 33734 filtered sentences\n"
     ]
    }
   ],
   "source": [
    "# From each cluster, select the sentence with fewest words\n",
    "filtered_sentences = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_sentences = [sentences[idx] for idx in cluster]\n",
    "    cluster_sentences = sorted(cluster_sentences, key=lambda x: len(x.split()))\n",
    "    filtered_sentences.append(cluster_sentences[0])\n",
    "\n",
    "print(f\"Collected {len(filtered_sentences)} filtered sentences\")\n",
    "sentences = filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk sentences by number of tokens\n",
    "sentences_by_num_tokens = defaultdict(list)\n",
    "for sentence in sentences:\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in nltk.word_tokenize(sentence) if token not in string.punctuation]\n",
    "    sentences_by_num_tokens[len(tokens)].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 filtered sentences\n"
     ]
    }
   ],
   "source": [
    "# Load word counts\n",
    "import pandas as pd\n",
    "word_counts = pd.read_csv(\"resources/count_1w.txt\", sep=\"\\t\", header=None)\n",
    "word_counts = word_counts.set_index(0)\n",
    "word_counts = word_counts.to_dict()[1]\n",
    "\n",
    "def minimum_word_count(sentence):\n",
    "    sentence = sentence.replace(\"'\", \"\")\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # remove stopwords\n",
    "    tokens = [token.lower() for token in tokens if token not in stopwords]\n",
    "    # remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return min([word_counts.get(token, 0) for token in tokens])\n",
    "\n",
    "# For each num_tokens_value, keep the top 200 sentences with highest minimum word count\n",
    "filtered_sentences_by_num_tokens = dict()\n",
    "for num_tokens_value, sentences_with_num_tokens in sorted(sentences_by_num_tokens.items(), key=lambda x: x[0]):\n",
    "    if num_tokens_value > 8 or num_tokens_value < 3:\n",
    "        continue\n",
    "\n",
    "    sentences_with_num_tokens = sorted(sentences_with_num_tokens, key=minimum_word_count, reverse=True)\n",
    "    filtered_sentences_by_num_tokens[num_tokens_value] = sentences_with_num_tokens[:200]\n",
    "\n",
    "print(f\"Collected {sum(len(sentences) for sentences in filtered_sentences_by_num_tokens.values())} filtered sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3\n",
      "Number of sentences: 200\n",
      "What new car? \n",
      " 273620358\n",
      "\n",
      "\n",
      "Number of tokens: 4\n",
      "Number of sentences: 200\n",
      "Does he help much? \n",
      " 210601244\n",
      "\n",
      "\n",
      "Number of tokens: 5\n",
      "Number of sentences: 200\n",
      "When would you have time? \n",
      " 360468339\n",
      "\n",
      "\n",
      "Number of tokens: 6\n",
      "Number of sentences: 200\n",
      "She was great in the part. \n",
      " 360468339\n",
      "\n",
      "\n",
      "Number of tokens: 7\n",
      "Number of sentences: 200\n",
      "Then you can have the other one. \n",
      " 216122487\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print random sentences from each num_tokens_value\n",
    "for num_tokens_value, sentences in filtered_sentences_by_num_tokens.items():\n",
    "    print(f\"Number of tokens: {num_tokens_value}\\nNumber of sentences: {len(sentences)}\")\n",
    "    print(random.choice(sentences), \"\\n\", minimum_word_count(random.choice(sentences)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5135f2d97ba4c708994744b74326a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building clusters:  63%|██████▎   | 33760/53779 [00:26<00:05, 3738.30it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/QwQ-32B-Preview\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Please translate into easy Chinese: \\\"{prompt}\\\"\"\n",
    "pinyin_template = \"\\\"{prompt}\\\" in Pinyin is\"\n",
    "\n",
    "def generate_response(prompt, template):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an experienced translator.\"},\n",
    "        {\"role\": \"user\", \"content\": template.format(prompt=prompt)}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/1000/75] Mandarin: “不是一个好的。”, Pinyin: “ bùshì yīgè hǎo de 。 ”                                 g dédào nàgè ne ？\r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pinyin_jyutping_sentence\n",
    "\n",
    "translated_sentence_records = []\n",
    "pattern = r\"Translation\\s*=\\s*([^\\]]+)[;|；|。]\\s*Pinyin\\s*=\\s*([^\\]]+)\"\n",
    "\n",
    "failure_count = 0\n",
    "\n",
    "sentences = list(set.union(*(set(sentences) for sentences in filtered_sentences_by_num_tokens.values())))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    translated_sentence = generate_response(sentence, translation_template).replace(\"\\n\", \" \")\n",
    "    translated_sentence = translated_sentence.strip('\"')\n",
    "    if len(translated_sentence) > 20:\n",
    "        failure_count += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    pinyin = pinyin_jyutping_sentence.pinyin(translated_sentence)\n",
    "    pinyin = pinyin.strip('\"')\n",
    "\n",
    "    #print(f\"Mandarin: {translated_sentence}\\tPinyin: {pinyin}\")\n",
    "\n",
    "    mandarin = translated_sentence.strip()\n",
    "    pinyin = pinyin.strip()\n",
    "    translated_sentence_records.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"mandarin\": mandarin,\n",
    "        \"pinyin\": pinyin\n",
    "    })\n",
    "    print(\" \" * 100, end=\"\\r\")\n",
    "    print(f\"[{i+1}/{len(sentences)}/{failure_count}] Mandarin: {mandarin}, Pinyin: {pinyin}\", end=\"\\r\")\n",
    "    #print(f\"Processed {i} sentences: Translation: {mandarin}, Pinyin: {pinyin}\", end=\"\\r\")\n",
    "\n",
    "translated_sentence_df = pd.DataFrame(translated_sentence_records)\n",
    "translated_sentence_df.to_csv(\"resources/translated_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import genanki\n",
    "\n",
    "model_id = random.randrange(1 << 30, 1 << 31)\n",
    "\n",
    "chinese_deck_model = genanki.Model(\n",
    "    model_id,\n",
    "    'Chinese Sentence Model',\n",
    "    fields=[\n",
    "        {'name': 'English'},\n",
    "        {'name': 'Mandarin'},\n",
    "        {'name': 'Pinyin'},\n",
    "        {'name': 'index'},\n",
    "    ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'English -> Mandarin',\n",
    "      'qfmt': '{{English}}',\n",
    "      # Show Mandarin and Pinyin side by side\n",
    "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{Mandarin}}<br>{{Pinyin}}',\n",
    "    },\n",
    "    {\n",
    "      'name': 'Mandarin -> English',\n",
    "      'qfmt': '{{Mandarin}}',\n",
    "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{Pinyin}}<br>{{English}}',\n",
    "    },\n",
    "  ])\n",
    "\n",
    "# Sort translated_sentence_df by length of Mandarin\n",
    "translated_sentence_df = translated_sentence_df.sort_values(by=\"mandarin\", key=lambda x: x.str.len())\n",
    "\n",
    "# Make notes for all rows in translated_sentence_df\n",
    "chinese_sentences_deck = genanki.Deck(\n",
    "    model_id + 1,\n",
    "    'Chinese Sentences'\n",
    ")\n",
    "\n",
    "for index, row in translated_sentence_df.iterrows():\n",
    "    chinese_sentences_deck.add_note(genanki.Note(\n",
    "        model=chinese_deck_model,\n",
    "        fields=[row[\"sentence\"], row[\"mandarin\"], row[\"pinyin\"], str(index)],\n",
    "        sort_field=\"index\"\n",
    "    ))\n",
    "\n",
    "genanki.Package(chinese_sentences_deck).write_to_file('chinese_sentences.apkg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
